# -*- coding: utf-8 -*-
"""SciSciNetEDA.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1LKYbEfOEbtKLLwNoFwNAU5BhUWsLXznt
"""

# !touch download.sh

# !bash download.sh

from google.colab import auth
auth.authenticate_user()
print('Authenticated')

# Configuration
PROJECT_ID = "sciscinet-mahdee"
BUCKET_NAME = "sciscinet-data"
DATASET_NAME = "SciSciNet"
TABLE_NAME = "SciSciNet_PaperAuthorAffiliations"
FILE_NAME = "SciSciNet_PaperAuthorAffiliations.tsv"
LOCATION = "US"

import time
from google.cloud import storage
from google.cloud import bigquery
from google.cloud.exceptions import Conflict, NotFound
import os
import sys

def check_file_exists():
    if not os.path.exists(FILE_NAME):
        print(f"Error: {FILE_NAME} not found. Please download it first.")
        sys.exit(1)

    print(f"Found {FILE_NAME} ({os.path.getsize(FILE_NAME) / (1024 * 1024 * 1024):.2f} GB)")
    return True

def upload_to_gcs():
    print(f"Uploading {FILE_NAME} to gs://{BUCKET_NAME}/...")

    storage_client = storage.Client(project=PROJECT_ID)

    try:
        bucket = storage_client.get_bucket(BUCKET_NAME)
        print(f"Bucket {BUCKET_NAME} already exists")
    except NotFound:
        print(f"Creating bucket {BUCKET_NAME}...")
        bucket = storage_client.create_bucket(BUCKET_NAME, location=LOCATION)

    blob = bucket.blob(FILE_NAME)

    if blob.exists():
        print(f"File already exists in bucket. Skipping upload.")
        return True

    start_time = time.time()
    file_size = os.path.getsize(FILE_NAME)

    def progress_callback(current):
        if current % (256 * 1024 * 1024) == 0:
            elapsed = time.time() - start_time
            speed = current / (1024 * 1024) / elapsed if elapsed > 0 else 0
            print(f"Uploaded {current / (1024 * 1024):.2f} MB / {file_size / (1024 * 1024):.2f} MB "
                  f"({current / file_size * 100:.2f}%) - {speed:.2f} MB/s")

    blob.upload_from_filename(
        FILE_NAME,
        content_type="text/tab-separated-values",
        timeout=None
    )

    end_time = time.time()
    print(f"Upload complete in {end_time - start_time:.2f} seconds")
    return True

check_file_exists()
upload_to_gcs()

!cat SciSciNet_PaperAuthorAffiliations.tsv | wc -l

def create_bigquery_table():
    """Create BigQuery dataset and table, load data from GCS for SciSciNet_PaperAuthorAffiliations"""
    print("Setting up BigQuery...")

    bq_client = bigquery.Client(project=PROJECT_ID)

    dataset_id = f"{PROJECT_ID}.{DATASET_NAME}"
    dataset = bigquery.Dataset(dataset_id)
    dataset.location = LOCATION

    try:
        bq_client.create_dataset(dataset, exists_ok=True)
        print(f"Dataset {DATASET_NAME} created or already exists")
    except Exception as e:
        print(f"Error creating dataset: {e}")
        return False

    schema = [
        bigquery.SchemaField("PaperID", "INTEGER", mode="REQUIRED", description="Unique identifier for each paper (Primary Key of the Paper table)"),
        bigquery.SchemaField("AuthorID", "INTEGER", mode="REQUIRED", description="Unique identifier for each author (Primary Key of the Author table)"),
        bigquery.SchemaField("AffiliationID", "FLOAT", description="Unique identifier for the author's affiliation (Primary Key of the Affiliation table)"),
        bigquery.SchemaField("AuthorSequenceNumber", "FLOAT", description="The sequence number indicating the author's order in the paper"),
    ]

    job_config = bigquery.LoadJobConfig(
        schema=schema,
        skip_leading_rows=1,
        source_format=bigquery.SourceFormat.CSV,
        field_delimiter='\t',
        allow_quoted_newlines=True,
        max_bad_records=10000,
        autodetect=True
    )

    uri = f"gs://{BUCKET_NAME}/{FILE_NAME}"

    table_ref = f"{dataset_id}.{TABLE_NAME}"

    print(f"Loading data from {uri} into {table_ref}...")
    print(f"This may take a while for a file with 413,869,502 lines...")

    load_job = bq_client.load_table_from_uri(
        uri, table_ref, job_config=job_config
    )

    try:
        load_job.result()

        table = bq_client.get_table(table_ref)
        print(f"Loaded {table.num_rows} rows into {table_ref}")
        return True

    except Exception as e:
        print(f"Error loading data: {e}")

        if hasattr(load_job, 'errors') and load_job.errors:
            for error in load_job.errors:
                print(f"Error details: {error}")

        return False

create_bigquery_table()

